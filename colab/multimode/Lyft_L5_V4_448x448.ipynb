{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyft L5 V4 448x448.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kool7/Lyft_Motion_Prediction_for_Autonomous_Vehicles_2020_Kaggle/blob/master/colab/multimode/Lyft_L5_V4_448x448.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYFrAu2fFfrw"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is the 4 version of previous notebooks which were used in [this](https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles/overview) competition Lyft Motion Prediction for Autonomous Vehicles\n",
        ". For full EDA please visit [here](https://www.kaggle.com/kool777/lyft-level5-eda-training-inference).\n",
        "\n",
        "  * Objective - Our task is to build motion prediction models. We need to predict the position of cars, cyclists and pedestrians in AV's environment. We need to predict the location of agents in next 50 frames.\n",
        "\n",
        "  * Tasks Performed :-\n",
        "    * Using new version of l5kit (1.1.0)\n",
        "\n",
        "This notebook is based on discussions [here](https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles/discussion/186492) and [here](https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles/discussion/187825)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo09lAQkE-wz"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NVbgGgCSdxE"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "We are going to download data from kaggle. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrYy8Sn3SbVm"
      },
      "source": [
        "import os \n",
        "\n",
        "os.makedirs('/content/lyft-motion-prediction-autonomous-vehicles/', exist_ok=True)\n",
        "os.chdir('/content/lyft-motion-prediction-autonomous-vehicles/')\n",
        "\n",
        "%cd '/content/lyft-motion-prediction-autonomous-vehicles/'\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVwSpM0ASiwN"
      },
      "source": [
        "os.makedirs('/content/lyft-motion-prediction-autonomous-vehicles/scenes/', exist_ok=True)\n",
        "os.chdir('/content/lyft-motion-prediction-autonomous-vehicles/scenes/')\n",
        "\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" --header=\"Cookie: _ga=GA1.3.1723755215.1594569279\" --header=\"Connection: keep-alive\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/19990/1472735/compressed/scenes.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1601530855&Signature=VUL5ViMOqoxt4ZberYrAxJsQTJmUd%2BNtPLKvi%2FYgw%2Fg3V9qzXQB%2Bp9320GKErjcqUmuHMgydxoiMZXDt8TNUCk%2BxpbQMGVPxwhOyVP0NqKMIi74YrKNbzjiLTabNtUSHEx1%2FgWDxtgNhkyNslAtT%2FENUV8zqZrABKX2Lo7VKv0f7s9r%2FjM%2BX4FMavjAgPc5YlHsSNGPwsVhNUraRPFSkeEpwakgLSq6uF4VhwV3bIhcSzsnxSZH9wLYZ5Lwj9rMryHNhmxTrQESENMkdht%2FAdPqCe4t6KOFgwe6r%2B6rWT%2BqcQaFK6fDqQHHw0gIj3VE4HNDkVqaV4ayzhuHIeNbBzQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dscenes.zip\" -c -O 'scenes.zip'\n",
        "!unzip ./scenes.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REoxKIELStjg"
      },
      "source": [
        "%cd '/content/lyft-motion-prediction-autonomous-vehicles/scenes/'\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oCkUO9FSuWz"
      },
      "source": [
        "%rm -r validate.zarr/\n",
        "%rm -r sample.zarr/\n",
        "%rm scenes.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6MmoxKwSwjy"
      },
      "source": [
        "%cd ..\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28jlNtkDSyZt"
      },
      "source": [
        "os.makedirs('/content/lyft-motion-prediction-autonomous-vehicles/aerial_map/', exist_ok=True)\n",
        "os.chdir('/content/lyft-motion-prediction-autonomous-vehicles/aerial_map/')\n",
        "\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" --header=\"Cookie: _ga=GA1.3.1723755215.1594569279\" --header=\"Connection: keep-alive\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/19990/1472735/compressed/aerial_map.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1601531043&Signature=QTkS13tw%2BItCroMzlv4SWj9%2F3e9LjKXgjdBw9ANbGAkBQMRIlRp1xD4eUlUBHwxUsw5l7VEIDjun0ndl%2FlIudvpMBwCHg%2BDCXWRrMslh9LWn7hFMulCoZLEO4oaou3LWs1vjCIk0%2BMSyLsWUBYvoqzTznSVm2mDhVEa%2B4sd%2FzfU%2FkLeJviUhA1%2BODufDNM%2B%2Bcyma%2FeLMF7FeKEoSce0Jslur1bQbhdroQZ%2FQ22PXTuDRd0JasgPkNvLRYH%2BDfCo3hVFE%2Bn176Wv59R3jg0Tymk2mZtZJCSMUnp2YaOR6sjqNKc3Rxk3RrAIaqXhwgkJebEQTiZhXip8wz9flmsV7cw%3D%3D&response-content-disposition=attachment%3B+filename%3Daerial_map.zip\" -c -O 'aerial_map.zip'\n",
        "!unzip ./aerial_map.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PL-zLtUS0UX"
      },
      "source": [
        "%rm -r aerial_map.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deg25CmxS2gs"
      },
      "source": [
        "%cd ..\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Asb5SpxS4OT"
      },
      "source": [
        "os.makedirs('/content/lyft-motion-prediction-autonomous-vehicles/semantic_map/', exist_ok=True)\n",
        "os.chdir('/content/lyft-motion-prediction-autonomous-vehicles/semantic_map/')\n",
        "\n",
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" --header=\"Cookie: _ga=GA1.3.1723755215.1594569279\" --header=\"Connection: keep-alive\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/19990/1472735/upload/semantic_map.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1601531085&Signature=KS3o7dPKQgF9ubQPtFYhYRft%2B84qbqczH4S1QdIOgYP2haztAtLTNYJ64xPKuWGH6XkcTVUV6%2F8vCCNNAPeENWi6B8%2Fy1LFaM46Orv%2FXg5b6779KVP4tG%2FI6d5w2sy3rLBSU3clZ7oKffhqxjsadCyEI4JCPz0Ub2l3U1OcDprdn3SYiAmkOOmpn8k%2F9sRgTOOfuMeEPMrXSSY8HdcDtnYwK3YiQqoe2qrWBDLOmvfUV1oYYBBZ1NwCykEJz%2B5U%2FcUq5ydOeBYQhvGW0o4MKFUqZDhGVGlqMufUs6n791UpymD1kzRlQRt7unux9jAoC5JTCR4bK0GIqojXqYiRtng%3D%3D&response-content-disposition=attachment%3B+filename%3Dsemantic_map.zip\" -c -O 'semantic_map.zip'\n",
        "!unzip ./semantic_map.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URx1otO8S8Yz"
      },
      "source": [
        "%rm -r semantic_map.zip\n",
        "%cd ..\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivPxuLK4S9tK"
      },
      "source": [
        "# Installing Dependencies\n",
        "\n",
        "We need to install some of the dependencies in order to process our data and train model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n20EXXkES_tE",
        "outputId": "0a325fda-8650-4549-82c8-f68adab90c21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# this script transports l5kit and dependencies\n",
        "!pip -q install pymap3d==2.1.0 \n",
        "!pip -q install protobuf==3.12.2 \n",
        "!pip -q install transforms3d \n",
        "!pip -q install zarr \n",
        "!pip -q install ptable\n",
        "\n",
        "# previous version 1.0.6\n",
        "!pip -q install --no-dependencies l5kit==1.0.6 "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for pymap3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 1.6MB/s \n",
            "\u001b[?25h  Building wheel for transforms3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 2.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8MB 21.6MB/s \n",
            "\u001b[?25h  Building wheel for zarr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ptable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYLJ5n4zWmmy"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8doy1hJUUdM",
        "outputId": "c2bc4aca-6df1-4f83-d511-1c99249116a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# import packages\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import gc, os, time\n",
        "from tqdm import tqdm\n",
        "from typing import Dict\n",
        "\n",
        "# deeplearning Pytorch\n",
        "import torch\n",
        "from torch import Tensor, nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import resnet18, resnet34, resnet50\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# L5 Toolkit\n",
        "import l5kit\n",
        "from l5kit.data import LocalDataManager, ChunkedDataset\n",
        "from l5kit.dataset import AgentDataset, EgoDataset\n",
        "from l5kit.rasterization import build_rasterizer\n",
        "\n",
        "print(f'L5 Toolkit Version : {l5kit.__version__}')\n",
        "print(f'Pytorch Version : {torch.__version__}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L5 Toolkit Version : 1.0.6\n",
            "Pytorch Version : 1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73VXrQ3rWz4P"
      },
      "source": [
        "# root directory\n",
        "INPUT_DIR = '/content/lyft-motion-prediction-autonomous-vehicles/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhR0J7nTX4k-"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5rJvsHJX3lt"
      },
      "source": [
        "cfg = {\n",
        "    'format_version': 4,\n",
        "    'model_params': {\n",
        "        'model_architecture': 'resnet18',\n",
        "        'history_num_frames': 10,\n",
        "        'history_step_size': 1,\n",
        "        'history_delta_time': 0.1,\n",
        "        'future_num_frames': 50,\n",
        "        'future_step_size': 1,\n",
        "        'future_delta_time': 0.1\n",
        "    },\n",
        "\n",
        "    'raster_params': {\n",
        "        'raster_size': [448, 448],\n",
        "        'pixel_size': [0.5, 0.5],\n",
        "        'ego_center': [0.25, 0.5],\n",
        "        'map_type': 'py_semantic',\n",
        "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
        "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
        "        'dataset_meta_key': 'meta.json',\n",
        "        'filter_agents_threshold': 0.5\n",
        "    },\n",
        "    \n",
        "    'train_data_loader': {\n",
        "        'key': 'scenes/train.zarr',\n",
        "        'batch_size': 64,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 0\n",
        "    },\n",
        "    \n",
        "    'train_params': {\n",
        "        'max_num_steps': 60000,\n",
        "        'image_coords': True,\n",
        "        'verbose': 100,\n",
        "    }\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na9-qZ_5ZUX6"
      },
      "source": [
        "# set env variable for data\n",
        "os.environ[\"L5KIT_DATA_FOLDER\"] = INPUT_DIR\n",
        "dm = LocalDataManager(None)\n",
        "\n",
        "# get config\n",
        "print(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdCQuloqaP-Q"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZACcgClMaQoC"
      },
      "source": [
        "def get_data(cfg):\n",
        "\n",
        "  '''\n",
        "  Description\n",
        "  ----------------\n",
        "  Prepare image data by \n",
        "  getting muti-channel tensors\n",
        "  and converting them to rgb images.\n",
        "  It iterates over other agents annotations\n",
        "  and passes the AgentDataset into Pytorch\n",
        "  Dataloader.\n",
        "\n",
        "  Arguments:\n",
        "  cfg -- Configuration file\n",
        "\n",
        "  Returns:\n",
        "  train_dataloader -- Pytorch dataloader\n",
        "  '''\n",
        "  # rasterizercfg, dm\n",
        "  rasterizer = build_rasterizer()\n",
        "\n",
        "  # Train dataset/dataloader\n",
        "  train_cfg = cfg[\"train_data_loader\"]\n",
        "  train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
        "  train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
        "  train_dataloader = DataLoader(train_dataset,\n",
        "                                shuffle=train_cfg[\"shuffle\"],\n",
        "                                batch_size=train_cfg[\"batch_size\"],\n",
        "                                num_workers=train_cfg[\"num_workers\"])\n",
        "\n",
        "  return train_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66TMlEJND7_r"
      },
      "source": [
        "# Loss Function: Negative Log Likelihood"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fatGqNZgD7Af"
      },
      "source": [
        "def negativeLogLikelihood(gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor):\n",
        "\n",
        "  '''\n",
        "  Description\n",
        "  -------------------------------\n",
        "  Compute Negative log-likelihood\n",
        "\n",
        "  Arguments:\n",
        "  ground_truth -- array of shape (time)x(2D coords)\n",
        "  prediction -- array of shape (modes)x(time)x(2D coords)\n",
        "  confidences -- array of shape (modes) with a confidence for each mode in each sample\n",
        "  avails -- array of shape (time) with the availability for each gt timestep\n",
        "\n",
        "  Returns:\n",
        "  error -- negative log-likelihood (floating number)\n",
        "  '''\n",
        "\n",
        "  assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
        "  batch_size, num_modes, future_len, num_coords = pred.shape\n",
        "\n",
        "  assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
        "  assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
        "  assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
        "  assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
        "  \n",
        "  # assert all data are valid\n",
        "  assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
        "  assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
        "  assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
        "  assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
        "\n",
        "  gt = torch.unsqueeze(gt, 1) \n",
        "  avails = avails[:, None, :, None]\n",
        "  error = torch.sum(((gt - pred) * avails) ** 2, dim=-1) \n",
        "\n",
        "  if cfg['train_params']['image_coords']:\n",
        "    error = error / 4\n",
        "\n",
        "  with np.errstate(divide=\"ignore\"): \n",
        "      error = torch.log(confidences) - 0.5 * torch.sum(error, dim = -1)\n",
        "\n",
        "  max_value, _ = error.max(dim = 1, keepdim = True) \n",
        "  error = -torch.log(torch.sum(torch.exp(error - max_value), dim = -1, keepdim=True)) - max_value\n",
        "  return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DyqS_aQEDDS"
      },
      "source": [
        "# Model: resnet 18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT_ZyHq8EElN"
      },
      "source": [
        "class LyftMultiModel(nn.Module):\n",
        "  \n",
        "  def __init__(self, cfg: Dict, num_modes=3):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    architecture = cfg[\"model_params\"][\"model_architecture\"]\n",
        "    backbone = eval(architecture)(pretrained=True, progress=True)\n",
        "    self.backbone = backbone\n",
        "\n",
        "    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
        "    num_in_channels = 3 + num_history_channels\n",
        "\n",
        "    self.backbone.conv1 = nn.Conv2d(\n",
        "        num_in_channels,\n",
        "        self.backbone.conv1.out_channels,\n",
        "        kernel_size=self.backbone.conv1.kernel_size,\n",
        "        stride=self.backbone.conv1.stride,\n",
        "        padding=self.backbone.conv1.padding,\n",
        "        bias=False,\n",
        "    )\n",
        "\n",
        "\n",
        "    if architecture == \"resnet50\":\n",
        "        backbone_out_features = 2048\n",
        "    else:\n",
        "        backbone_out_features = 512\n",
        "\n",
        "    # X, Y coords for the future positions (output shape: batch_sizex50x2)\n",
        "    self.future_len = cfg[\"model_params\"][\"future_num_frames\"]\n",
        "    num_targets = 2 * self.future_len\n",
        "\n",
        "    # You can add more layers here.\n",
        "    self.head = nn.Sequential(\n",
        "        # nn.Dropout(0.2),\n",
        "        nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
        "    )\n",
        "\n",
        "    self.num_preds = num_targets * num_modes\n",
        "    self.num_modes = num_modes\n",
        "\n",
        "    self.logit = nn.Linear(4096, out_features=self.num_preds + num_modes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.backbone.conv1(x)\n",
        "    x = self.backbone.bn1(x)\n",
        "    x = self.backbone.relu(x)\n",
        "    x = self.backbone.maxpool(x)\n",
        "\n",
        "    x = self.backbone.layer1(x)\n",
        "    x = self.backbone.layer2(x)\n",
        "    x = self.backbone.layer3(x)\n",
        "    x = self.backbone.layer4(x)\n",
        "\n",
        "    x = self.backbone.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "\n",
        "    x = self.head(x)\n",
        "    x = self.logit(x)\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2aan49I4jqX"
      },
      "source": [
        "# Used in V3\n",
        "def forward(data, model, device, criterion = negativeLogLikelihood):\n",
        "\n",
        "  # USE GPU FOR TRAINIG\n",
        "  inputs = data[\"image\"].to(device)\n",
        "  target_availabilities = data[\"target_availabilities\"].to(device)\n",
        "  targets = data[\"target_positions\"].to(device)\n",
        "  matrix = data[\"world_to_image\"].to(device)\n",
        "  centroid = data[\"centroid\"].to(device)[:,None,:].to(torch.float)\n",
        "\n",
        "  # FORWARD PASS TO GET OUTPUT/LOGITS\n",
        "  outputs = model(inputs)\n",
        "\n",
        "  bs,tl,_ = targets.shape\n",
        "  assert tl == cfg[\"model_params\"][\"future_num_frames\"]\n",
        "\n",
        "  if cfg['train_params']['image_coords']:\n",
        "      targets = targets + centroid\n",
        "      targets = torch.cat([targets,torch.ones((bs,tl,1)).to(device)], dim=2)\n",
        "      targets = torch.matmul(matrix.to(torch.float), targets.transpose(1,2))\n",
        "      targets = targets.transpose(1,2)[:,:,:2]\n",
        "      bias = torch.tensor([56.25, 112.5])[None,None,:].to(device)\n",
        "      targets = targets - bias\n",
        "\n",
        "  confidences, pred = outputs[:,:3], outputs[:,3:]\n",
        "  pred = pred.view(bs, 3, tl, 2)\n",
        "  assert confidences.shape == (bs, 3)\n",
        "  confidences = torch.softmax(confidences, dim=1)\n",
        "\n",
        "  # CALCULATE LOSS: NEGATIVE LOG LIKELIHOOD\n",
        "  loss = criterion(targets, pred, confidences, target_availabilities)\n",
        "  loss = torch.mean(loss)\n",
        "\n",
        "  if cfg['train_params']['image_coords']:\n",
        "      matrix_inv = torch.inverse(matrix)\n",
        "      pred = pred + bias[:,None,:,:]\n",
        "      pred = torch.cat([pred,torch.ones((bs,3,tl,1)).to(device)], dim=3)\n",
        "      pred = torch.stack([torch.matmul(matrix_inv.to(torch.float), pred[:,i].transpose(1,2)) \n",
        "                          for i in range(3)], dim=1)\n",
        "      pred = pred.transpose(2,3)[:,:,:,:2]\n",
        "      pred = pred - centroid[:,None,:,:]\n",
        "\n",
        "  return loss, pred, confidences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "segKq6UeadH-"
      },
      "source": [
        "# INSTANTIATE MODEL CLASS\n",
        "model = LyftMultiModel(cfg)\n",
        "\n",
        "# USE GPU FOR TRAINING MODEL\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# GPU FOR MODEL\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESyAw4KasqNa"
      },
      "source": [
        "# Checkpoint\n",
        "\n",
        "Run this cell when you want to resume  training your model after certain number of epochs. Uncomment all the lines if you have saved weights earlier to resume training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vst3EJ-ScrDh"
      },
      "source": [
        "# WEIGHT_FILE = '/content/drive/My Drive/Kaggle/Lyft L5 Motion Prediction/resnet18_448x448_model_state_v3_1400.pth'\n",
        "# checkpoint = torch.load(WEIGHT_FILE, map_location=device)\n",
        "# epoch = checkpoint['epoch']\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bs2RpyrtEkv"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUdbWBNWs3rb"
      },
      "source": [
        "def train(cfg, device, model, optimizer):\n",
        "\n",
        "  losses_train = []\n",
        "  iterations = []\n",
        "  metrics = []\n",
        "  times = []\n",
        "\n",
        "  print('getting Dataloader...')\n",
        "  train_dataloader = get_data(cfg)\n",
        "\n",
        "  progress_bar = tqdm(range(epoch, cfg['train_params']['max_num_steps']))\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  print('Training...')\n",
        "  for i in progress_bar:\n",
        "    try:\n",
        "      data = next(iter(train_dataloader))\n",
        "    except StopIteration:\n",
        "      data = next(iter(train_dataloader))\n",
        "\n",
        "    torch.set_grade_enabled(True)\n",
        "\n",
        "    # CLEAR GRADIENTS W.R.T TO PARAMETERS\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # FORWARD PASS TO GET OUTPUT\n",
        "    loss, _, _ = forward(data, model, device)\n",
        "\n",
        "    # GETTING GRADIENTS W.R.T TO PARAMETERS\n",
        "    loss.backward()\n",
        "\n",
        "    # UPDATING PARAMETERS\n",
        "    optimizer.step()\n",
        "\n",
        "    # Appending losses in a list\n",
        "    losses_train.append(loss.item())\n",
        "\n",
        "    if i % cfg['train_params']['verbose'] == 0:\n",
        "      torch.save({'epoch': i + 1,\n",
        "                  'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict()},\n",
        "                 f'/content/drive/My Drive/Kaggle/Lyft L5 Motion Prediction/resnet18_448x448_model_state_v4_{i}.pth')\n",
        "      progress_bar.set_description(f'loss: {loss.item()} loss_avg: {np.mean(losses_train)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF1d1ufiy0WA"
      },
      "source": [
        "# Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9mYGlBjyo1U"
      },
      "source": [
        "train(cfg, device, model, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}